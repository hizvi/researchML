{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_edus import EDUSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> DATA LOADED\n"
     ]
    }
   ],
   "source": [
    "data = EDUSample()\n",
    "path = r'../LabeledEDUS_final.txt'\n",
    "\n",
    "edus, targets = data.read_labeled(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = r'../../../GloVe/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize edus\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(edus)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_edus = t.texts_to_sequences(edus)\n",
    "\n",
    "# max_length = 1 << (max(map(lambda x: len(x), encoded_edus)) - 1).bit_length()  # pad to smallest power of 2 greater than the largest edu\n",
    "max_length = sum(map(lambda x: len(x), encoded_edus)) // len(encoded_edus)       # pad to the average length of edus\n",
    "padded_edus = pad_sequences(encoded_edus, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = data.split_data(padded_edus, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open(GLOVE_PATH)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regularization \n",
    "regularizer = None\n",
    "regularization = True\n",
    "dropout = True\n",
    "if regularization:\n",
    "    regularizer = l1(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup\n",
      "Fitting model\n",
      "Epoch 1/100\n",
      "2266/2266 [==============================] - 2s 753us/step - loss: 0.7152 - acc: 0.5300\n",
      "Epoch 2/100\n",
      "2266/2266 [==============================] - 0s 134us/step - loss: 0.6704 - acc: 0.6161\n",
      "Epoch 3/100\n",
      "2266/2266 [==============================] - 0s 141us/step - loss: 0.6219 - acc: 0.6726\n",
      "Epoch 4/100\n",
      "2266/2266 [==============================] - 0s 133us/step - loss: 0.5856 - acc: 0.7070\n",
      "Epoch 5/100\n",
      "2266/2266 [==============================] - 0s 135us/step - loss: 0.5631 - acc: 0.7202\n",
      "Epoch 6/100\n",
      "2266/2266 [==============================] - 0s 140us/step - loss: 0.5476 - acc: 0.7370\n",
      "Epoch 7/100\n",
      "2266/2266 [==============================] - 0s 137us/step - loss: 0.5385 - acc: 0.7458\n",
      "Epoch 8/100\n",
      "2266/2266 [==============================] - 0s 133us/step - loss: 0.5288 - acc: 0.7639\n",
      "Epoch 9/100\n",
      "2266/2266 [==============================] - 0s 138us/step - loss: 0.5165 - acc: 0.7643\n",
      "Epoch 10/100\n",
      "2266/2266 [==============================] - 0s 158us/step - loss: 0.5095 - acc: 0.7674\n",
      "Epoch 11/100\n",
      "2266/2266 [==============================] - 0s 183us/step - loss: 0.5012 - acc: 0.7807\n",
      "Epoch 12/100\n",
      "2266/2266 [==============================] - 0s 152us/step - loss: 0.5020 - acc: 0.7763\n",
      "Epoch 13/100\n",
      "2266/2266 [==============================] - 0s 154us/step - loss: 0.4975 - acc: 0.7780\n",
      "Epoch 14/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.4819 - acc: 0.7895\n",
      "Epoch 15/100\n",
      "2266/2266 [==============================] - 0s 131us/step - loss: 0.4746 - acc: 0.7952\n",
      "Epoch 16/100\n",
      "2266/2266 [==============================] - 0s 128us/step - loss: 0.5146 - acc: 0.7665\n",
      "Epoch 17/100\n",
      "2266/2266 [==============================] - 0s 136us/step - loss: 0.4670 - acc: 0.7966\n",
      "Epoch 18/100\n",
      "2266/2266 [==============================] - 0s 143us/step - loss: 0.4744 - acc: 0.7895\n",
      "Epoch 19/100\n",
      "2266/2266 [==============================] - 0s 151us/step - loss: 0.4614 - acc: 0.7904\n",
      "Epoch 20/100\n",
      "2266/2266 [==============================] - 0s 144us/step - loss: 0.4611 - acc: 0.7988\n",
      "Epoch 21/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.4639 - acc: 0.8027\n",
      "Epoch 22/100\n",
      "2266/2266 [==============================] - 0s 143us/step - loss: 0.4674 - acc: 0.8010\n",
      "Epoch 23/100\n",
      "2266/2266 [==============================] - 0s 140us/step - loss: 0.4509 - acc: 0.8151\n",
      "Epoch 24/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.4520 - acc: 0.8124\n",
      "Epoch 25/100\n",
      "2266/2266 [==============================] - 0s 158us/step - loss: 0.4546 - acc: 0.8116\n",
      "Epoch 26/100\n",
      "2266/2266 [==============================] - 0s 121us/step - loss: 0.4402 - acc: 0.8155\n",
      "Epoch 27/100\n",
      "2266/2266 [==============================] - 0s 126us/step - loss: 0.4398 - acc: 0.8133\n",
      "Epoch 28/100\n",
      "2266/2266 [==============================] - 0s 131us/step - loss: 0.4383 - acc: 0.8142\n",
      "Epoch 29/100\n",
      "2266/2266 [==============================] - 0s 131us/step - loss: 0.4311 - acc: 0.8244\n",
      "Epoch 30/100\n",
      "2266/2266 [==============================] - 0s 124us/step - loss: 0.4268 - acc: 0.8239\n",
      "Epoch 31/100\n",
      "2266/2266 [==============================] - 0s 125us/step - loss: 0.4337 - acc: 0.8204\n",
      "Epoch 32/100\n",
      "2266/2266 [==============================] - 0s 123us/step - loss: 0.4360 - acc: 0.8266\n",
      "Epoch 33/100\n",
      "2266/2266 [==============================] - 0s 153us/step - loss: 0.4275 - acc: 0.8239\n",
      "Epoch 34/100\n",
      "2266/2266 [==============================] - 0s 121us/step - loss: 0.4186 - acc: 0.8323\n",
      "Epoch 35/100\n",
      "2266/2266 [==============================] - 0s 123us/step - loss: 0.4246 - acc: 0.8323\n",
      "Epoch 36/100\n",
      "2266/2266 [==============================] - 0s 124us/step - loss: 0.4164 - acc: 0.8402\n",
      "Epoch 37/100\n",
      "2266/2266 [==============================] - 0s 121us/step - loss: 0.4175 - acc: 0.8380\n",
      "Epoch 38/100\n",
      "2266/2266 [==============================] - 0s 123us/step - loss: 0.4159 - acc: 0.8372\n",
      "Epoch 39/100\n",
      "2266/2266 [==============================] - 0s 124us/step - loss: 0.4112 - acc: 0.8398\n",
      "Epoch 40/100\n",
      "2266/2266 [==============================] - 0s 138us/step - loss: 0.4107 - acc: 0.8367\n",
      "Epoch 41/100\n",
      "2266/2266 [==============================] - 0s 150us/step - loss: 0.4294 - acc: 0.8270\n",
      "Epoch 42/100\n",
      "2266/2266 [==============================] - 0s 139us/step - loss: 0.4084 - acc: 0.8367\n",
      "Epoch 43/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.3965 - acc: 0.8473\n",
      "Epoch 44/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3997 - acc: 0.8442\n",
      "Epoch 45/100\n",
      "2266/2266 [==============================] - 0s 165us/step - loss: 0.4052 - acc: 0.8372\n",
      "Epoch 46/100\n",
      "2266/2266 [==============================] - 0s 148us/step - loss: 0.4005 - acc: 0.8407\n",
      "Epoch 47/100\n",
      "2266/2266 [==============================] - 0s 161us/step - loss: 0.4077 - acc: 0.8442\n",
      "Epoch 48/100\n",
      "2266/2266 [==============================] - 0s 174us/step - loss: 0.4059 - acc: 0.8464\n",
      "Epoch 49/100\n",
      "2266/2266 [==============================] - 0s 167us/step - loss: 0.3993 - acc: 0.8455\n",
      "Epoch 50/100\n",
      "2266/2266 [==============================] - 0s 183us/step - loss: 0.3941 - acc: 0.8464\n",
      "Epoch 51/100\n",
      "2266/2266 [==============================] - 0s 155us/step - loss: 0.3951 - acc: 0.8425\n",
      "Epoch 52/100\n",
      "2266/2266 [==============================] - 0s 165us/step - loss: 0.3943 - acc: 0.8508\n",
      "Epoch 53/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3831 - acc: 0.8614\n",
      "Epoch 54/100\n",
      "2266/2266 [==============================] - 0s 144us/step - loss: 0.3874 - acc: 0.8530\n",
      "Epoch 55/100\n",
      "2266/2266 [==============================] - 0s 142us/step - loss: 0.3859 - acc: 0.8575\n",
      "Epoch 56/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.3783 - acc: 0.8610\n",
      "Epoch 57/100\n",
      "2266/2266 [==============================] - 0s 177us/step - loss: 0.3833 - acc: 0.8553\n",
      "Epoch 58/100\n",
      "2266/2266 [==============================] - 0s 169us/step - loss: 0.3874 - acc: 0.8451\n",
      "Epoch 59/100\n",
      "2266/2266 [==============================] - 0s 155us/step - loss: 0.3826 - acc: 0.8504\n",
      "Epoch 60/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3854 - acc: 0.8597\n",
      "Epoch 61/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3844 - acc: 0.8522\n",
      "Epoch 62/100\n",
      "2266/2266 [==============================] - 0s 142us/step - loss: 0.3782 - acc: 0.8588\n",
      "Epoch 63/100\n",
      "2266/2266 [==============================] - 0s 147us/step - loss: 0.3745 - acc: 0.8628\n",
      "Epoch 64/100\n",
      "2266/2266 [==============================] - 0s 147us/step - loss: 0.3673 - acc: 0.8667\n",
      "Epoch 65/100\n",
      "2266/2266 [==============================] - 0s 167us/step - loss: 0.3688 - acc: 0.8623\n",
      "Epoch 66/100\n",
      "2266/2266 [==============================] - 0s 148us/step - loss: 0.3744 - acc: 0.8561\n",
      "Epoch 67/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.3752 - acc: 0.8588\n",
      "Epoch 68/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3675 - acc: 0.8579\n",
      "Epoch 69/100\n",
      "2266/2266 [==============================] - 0s 149us/step - loss: 0.3746 - acc: 0.8561\n",
      "Epoch 70/100\n",
      "2266/2266 [==============================] - 0s 170us/step - loss: 0.3618 - acc: 0.8685\n",
      "Epoch 71/100\n",
      "2266/2266 [==============================] - 0s 142us/step - loss: 0.3739 - acc: 0.8641\n",
      "Epoch 72/100\n",
      "2266/2266 [==============================] - 0s 153us/step - loss: 0.3644 - acc: 0.8588\n",
      "Epoch 73/100\n",
      "2266/2266 [==============================] - 0s 172us/step - loss: 0.3639 - acc: 0.8667\n",
      "Epoch 74/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3699 - acc: 0.8614\n",
      "Epoch 75/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.3531 - acc: 0.8733\n",
      "Epoch 76/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3463 - acc: 0.8738\n",
      "Epoch 77/100\n",
      "2266/2266 [==============================] - 0s 143us/step - loss: 0.3526 - acc: 0.8747\n",
      "Epoch 78/100\n",
      "2266/2266 [==============================] - 0s 168us/step - loss: 0.3598 - acc: 0.8711\n",
      "Epoch 79/100\n",
      "2266/2266 [==============================] - 0s 175us/step - loss: 0.3451 - acc: 0.8751\n",
      "Epoch 80/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.3472 - acc: 0.8689\n",
      "Epoch 81/100\n",
      "2266/2266 [==============================] - 0s 174us/step - loss: 0.3429 - acc: 0.8658\n",
      "Epoch 82/100\n",
      "2266/2266 [==============================] - 0s 150us/step - loss: 0.3518 - acc: 0.8716\n",
      "Epoch 83/100\n",
      "2266/2266 [==============================] - 0s 143us/step - loss: 0.3421 - acc: 0.8742\n",
      "Epoch 84/100\n",
      "2266/2266 [==============================] - 0s 173us/step - loss: 0.3418 - acc: 0.8725\n",
      "Epoch 85/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3306 - acc: 0.8839\n",
      "Epoch 86/100\n",
      "2266/2266 [==============================] - 0s 142us/step - loss: 0.3384 - acc: 0.8813\n",
      "Epoch 87/100\n",
      "2266/2266 [==============================] - 0s 147us/step - loss: 0.3367 - acc: 0.8751\n",
      "Epoch 88/100\n",
      "2266/2266 [==============================] - 0s 159us/step - loss: 0.3434 - acc: 0.8725\n",
      "Epoch 89/100\n",
      "2266/2266 [==============================] - 0s 158us/step - loss: 0.3540 - acc: 0.8632\n",
      "Epoch 90/100\n",
      "2266/2266 [==============================] - 0s 152us/step - loss: 0.3284 - acc: 0.8831\n",
      "Epoch 91/100\n",
      "2266/2266 [==============================] - 0s 189us/step - loss: 0.3240 - acc: 0.8901\n",
      "Epoch 92/100\n",
      "2266/2266 [==============================] - 0s 146us/step - loss: 0.3340 - acc: 0.8861\n",
      "Epoch 93/100\n",
      "2266/2266 [==============================] - 0s 151us/step - loss: 0.3268 - acc: 0.8808\n",
      "Epoch 94/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3263 - acc: 0.8861\n",
      "Epoch 95/100\n",
      "2266/2266 [==============================] - 0s 141us/step - loss: 0.3325 - acc: 0.8769\n",
      "Epoch 96/100\n",
      "2266/2266 [==============================] - 0s 145us/step - loss: 0.3266 - acc: 0.8853\n",
      "Epoch 97/100\n",
      "2266/2266 [==============================] - 0s 144us/step - loss: 0.3190 - acc: 0.8928\n",
      "Epoch 98/100\n",
      "2266/2266 [==============================] - 0s 147us/step - loss: 0.3179 - acc: 0.8822\n",
      "Epoch 99/100\n",
      "2266/2266 [==============================] - 0s 149us/step - loss: 0.3201 - acc: 0.8888\n",
      "Epoch 100/100\n",
      "2266/2266 [==============================] - 0s 163us/step - loss: 0.3132 - acc: 0.8963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f06299f0588>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model Setup') \n",
    "\n",
    "inputs = Input(shape=(max_length,), dtype='int32')\n",
    "embed_sequence = Embedding(vocab_size, 100, \n",
    "                           #weights=[embedding_matrix],\n",
    "                           embeddings_initializer=Constant(embedding_matrix), \n",
    "                           input_length=max_length,\n",
    "                           trainable=False)(inputs)\n",
    "\n",
    "x = Dense(256, activation='sigmoid', kernel_regularizer=regularizer)(embed_sequence)\n",
    "\n",
    "if dropout:\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "model = Model(inputs, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print('Fitting model')\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=100,\n",
    "          batch_size=None\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_test)\n",
    "results = np.array([0 if r < 0.5 else 1 for r in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133/1133 [==============================] - 1s 589us/step\n",
      "2266/2266 [==============================] - 0s 52us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6656858978919613, 0.7511032659820193],\n",
       " [0.2725239859307012, 0.9223300971399865])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m = model.evaluate(X_test, y_test)\n",
    "model.evaluate(X_test, y_test), model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7475728155339806"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyson/.conda/envs/iitml/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2612533097969991, 0.5, 0.34318840579710147, None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_test, results, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(model.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 211.00 337.00\" width=\"211pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 207,-333 207,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139665885484368 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139665885484368</title>\n",
       "<polygon fill=\"none\" points=\"21.5,-292.5 21.5,-328.5 181.5,-328.5 181.5,-292.5 21.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-306.8\">input_3: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139665885484704 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139665885484704</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 203,-255.5 203,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-233.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 139665885484368&#45;&gt;139665885484704 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139665885484368-&gt;139665885484704</title>\n",
       "<path d=\"M101.5,-292.4551C101.5,-284.3828 101.5,-274.6764 101.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"105.0001,-265.5903 101.5,-255.5904 98.0001,-265.5904 105.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139665885484592 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139665885484592</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-146.5 37.5,-182.5 165.5,-182.5 165.5,-146.5 37.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139665885484704&#45;&gt;139665885484592 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139665885484704-&gt;139665885484592</title>\n",
       "<path d=\"M101.5,-219.4551C101.5,-211.3828 101.5,-201.6764 101.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"105.0001,-192.5903 101.5,-182.5904 98.0001,-192.5904 105.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139665885503728 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139665885503728</title>\n",
       "<polygon fill=\"none\" points=\"31,-73.5 31,-109.5 172,-109.5 172,-73.5 31,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-87.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 139665885484592&#45;&gt;139665885503728 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139665885484592-&gt;139665885503728</title>\n",
       "<path d=\"M101.5,-146.4551C101.5,-138.3828 101.5,-128.6764 101.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"105.0001,-119.5903 101.5,-109.5904 98.0001,-119.5904 105.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139665885485040 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139665885485040</title>\n",
       "<polygon fill=\"none\" points=\"21,-.5 21,-36.5 182,-36.5 182,-.5 21,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-14.8\">output_layer: Dense</text>\n",
       "</g>\n",
       "<!-- 139665885503728&#45;&gt;139665885485040 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139665885503728-&gt;139665885485040</title>\n",
       "<path d=\"M101.5,-73.4551C101.5,-65.3828 101.5,-55.6764 101.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"105.0001,-46.5903 101.5,-36.5904 98.0001,-46.5904 105.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
